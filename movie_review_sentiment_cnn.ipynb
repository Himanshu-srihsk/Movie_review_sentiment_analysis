{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string \n",
    "import re \n",
    "from os import listdir \n",
    "from collections import Counter \n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "from keras.utils.vis_utils import plot_model \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten \n",
    "from pandas import DataFrame \n",
    "from matplotlib import pyplot\n",
    "from keras.layers import Embedding \n",
    "from keras.layers.convolutional import Conv1D \n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory \n",
    "def load_doc(filename): \n",
    "    # open the file as read only \n",
    "    file = open(filename,'r') \n",
    "    # read all text \n",
    "    text = file.read() \n",
    "    # close the file \n",
    "    file.close() \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn a doc into clean tokens \n",
    "def clean_doc(doc,vocab): \n",
    "    # split into tokens by white space \n",
    "    tokens = doc.split() \n",
    "    # prepare regex for char filtering \n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation)) \n",
    "    # remove punctuation from each word \n",
    "    tokens = [re_punc.sub('', w) for w in tokens] \n",
    "    return ' '.join(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc, clean and return line of tokens \n",
    "def doc_to_line(filename, vocab): \n",
    "    # load the doc \n",
    "    doc = load_doc(filename) \n",
    "    # clean doc \n",
    "    tokens = clean_doc(doc,vocab) \n",
    "    # filter by vocab \n",
    "    tokens = [w for w in tokens if w in vocab] \n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load all docs in a directory \n",
    "def process_docs(directory, vocab,is_train): \n",
    "    lines = list()\n",
    "    # walk through all files in the folder \n",
    "    for filename in listdir(directory): \n",
    "        # skip any reviews in the test set \n",
    "        if is_train and filename.startswith('cv9'): \n",
    "            continue \n",
    "        if not is_train and not filename.startswith('cv9'): \n",
    "            continue     \n",
    "        # create the full path of the file to open \n",
    "        path = directory + '/' + filename \n",
    "        # load the doc \n",
    "        doc = load_doc(path) \n",
    "        # clean doc \n",
    "        tokens = clean_doc(doc,vocab) \n",
    "        lines.append(tokens)\n",
    "    return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load and clean a dataset \n",
    "def load_clean_dataset(vocab,is_train): \n",
    "    # load documents \n",
    "    neg = process_docs('txt_sentoken/neg', vocab,is_train) \n",
    "    pos = process_docs('txt_sentoken/pos', vocab,is_train) \n",
    "    docs = neg + pos \n",
    "    # prepare labels \n",
    "    labels = [0 for _ in range(len(neg))] + [1 for _ in range(len(pos))] \n",
    "    return docs, labels    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a tokenizer \n",
    "def create_tokenizer(lines): \n",
    "    tokenizer = Tokenizer() \n",
    "    tokenizer.fit_on_texts(lines) \n",
    "    return tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode and pad documents \n",
    "def encode_docs(tokenizer, max_length, docs): \n",
    "    # integer encode \n",
    "    encoded = tokenizer.texts_to_sequences(docs) \n",
    "    # pad sequences \n",
    "    padded = pad_sequences(encoded, maxlen=max_length, padding='post') \n",
    "    return padded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "def define_model(vocab_size, max_length): \n",
    "    # define network \n",
    "    model = Sequential() \n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length)) \n",
    "    model.add(Conv1D(filters=32, kernel_size=8, activation='relu')) \n",
    "    model.add(MaxPooling1D(pool_size=2)) \n",
    "    model.add(Flatten()) \n",
    "    model.add(Dense(10, activation='relu')) \n",
    "    model.add(Dense(1, activation='sigmoid')) \n",
    "    # compile network \n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "    # summarize defined \n",
    "    model.summary() \n",
    "    plot_model(model, to_file='model_cnn.png', show_shapes=True) \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify a review as negative or positive \n",
    "def predict_sentiment(review, vocab, tokenizer, max_length, model): \n",
    "    # clean review \n",
    "    line = clean_doc(review, vocab) \n",
    "    # encode and pad review \n",
    "    padded = encode_docs(tokenizer, max_length, [line]) \n",
    "    # predict sentiment \n",
    "    yhat = model.predict(padded, verbose=0) \n",
    "    # retrieve predicted percentage and label \n",
    "    percent_pos = yhat[0,0] \n",
    "    if round(percent_pos) == 0: \n",
    "        return (1-percent_pos), 'NEGATIVE' \n",
    "    return percent_pos, 'POSITIVE'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 45242\n",
      "Maximum length: 2365\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2365, 100)         4524200   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 2358, 32)          25632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 1179, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 37728)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                377290    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 4,927,133\n",
      "Trainable params: 4,927,133\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 19s - loss: 0.6896 - acc: 0.5350\n",
      "Epoch 2/10\n",
      " - 19s - loss: 0.6367 - acc: 0.6233\n",
      "Epoch 3/10\n",
      " - 19s - loss: 0.2924 - acc: 0.8839\n",
      "Epoch 4/10\n",
      " - 19s - loss: 0.0420 - acc: 0.9928\n",
      "Epoch 5/10\n",
      " - 19s - loss: 0.0059 - acc: 1.0000\n",
      "Epoch 6/10\n",
      " - 20s - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 7/10\n",
      " - 20s - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 8/10\n",
      " - 20s - loss: 9.2099e-04 - acc: 1.0000\n",
      "Epoch 9/10\n",
      " - 20s - loss: 7.1027e-04 - acc: 1.0000\n",
      "Epoch 10/10\n",
      " - 20s - loss: 5.6658e-04 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary \n",
    "vocab_filename = 'vocab.txt' \n",
    "vocab = load_doc(vocab_filename) \n",
    "vocab = set(vocab.split()) \n",
    "# load training data \n",
    "train_docs, ytrain = load_clean_dataset(vocab, True) \n",
    "# create the tokenizer \n",
    "tokenizer = create_tokenizer(train_docs) \n",
    "# define vocabulary size \n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "# calculate the maximum sequence length \n",
    "max_length = max([len(s.split()) for s in train_docs]) \n",
    "print('Maximum length: %d' % max_length) \n",
    "# encode data \n",
    "Xtrain = encode_docs(tokenizer, max_length, train_docs) \n",
    "# define model \n",
    "model = define_model(vocab_size, max_length) \n",
    "# fit network \n",
    "model.fit(Xtrain, ytrain, epochs=10, verbose=2) \n",
    "# save the model \n",
    "model.save('model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 100.00\n",
      "Test Accuracy: 89.00\n"
     ]
    }
   ],
   "source": [
    "test_docs, ytest = load_clean_dataset(vocab, False)\n",
    "Xtest = encode_docs(tokenizer, max_length, test_docs) \n",
    "# load the model \n",
    "model = load_model('model_cnn.h5') \n",
    "# evaluate model on training dataset \n",
    "_, acc = model.evaluate(Xtrain, ytrain, verbose=0) \n",
    "print('Train Accuracy: %.2f' % (acc*100)) \n",
    "# evaluate model on test dataset \n",
    "_, acc = model.evaluate(Xtest, ytest, verbose=0) \n",
    "print('Test Accuracy: %.2f' % (acc*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [Everyone will enjoy this film. I love it, recommended!]\n",
      "Sentiment: POSITIVE (55.834%)\n",
      "Review: [This is a bad movie. Do not watch it. It sucks.]\n",
      "Sentiment: NEGATIVE (60.949%)\n"
     ]
    }
   ],
   "source": [
    "text = 'Everyone will enjoy this film. I love it, recommended!' \n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100)) \n",
    "# test negative text \n",
    "text = 'This is a bad movie. Do not watch it. It sucks.' \n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer, max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [It was nice ! i enjoyed it]\n",
      "Sentiment: POSITIVE (51.938%)\n"
     ]
    }
   ],
   "source": [
    "text = 'It was nice ! i enjoyed it' \n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer,max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: [I hate the movvie!]\n",
      "Sentiment: NEGATIVE (50.493%)\n"
     ]
    }
   ],
   "source": [
    "text = 'I hate the movvie!' \n",
    "percent, sentiment = predict_sentiment(text, vocab, tokenizer,max_length, model) \n",
    "print('Review: [%s]\\nSentiment: %s (%.3f%%)' % (text, sentiment, percent*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
